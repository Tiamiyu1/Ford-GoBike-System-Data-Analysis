{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering\n",
    "\n",
    "1. `twitter_archive_enhanced.csv`  \n",
    "This data was provided in the project guideline. I downloaded it and uploaded it to my workspace by clicking the `jupyter` icon then upload. I imported the python `pandas` library as pd and used the pandas read_csv() function to read the file into a dataframe named `df`.\n",
    "\n",
    "\n",
    "2. `image-predictions.tsv`                                                                                                     I imported the python `requests` and `os` libraries. With the get() function of the requests library, I got the data through its url and saved it in a response variable. Response displayed `200`, meaning that it was successful.\n",
    "    \n",
    "    Using the python `with open` function, I wrote the response's content to a `tsv` file in the same working directory. I then read the downloaded tsv file into a datframe named `df2`.\n",
    "\n",
    "\n",
    "3. `tweet_json.txt`\n",
    "\n",
    "    I created a twitter developer account and created an application for the project. I used the app credentials (consumer_key, consumer_secret, access_token and access_secret)for the twitter api authentication.\n",
    "    I imported `tweepy` and `json`, authenticated tweepy.OAuthHandler and set `wait_on_rate_limit` and `wait_on_rate_limit_notify` to `True` in the api parameter in order to notify me and wait after tweet limit (900) and continue automatically at the end of the waiting time.\n",
    "    I set the needed tweet id to scrap online from the tweet given in the first dataset, created an empty dictionary to save failed tweets and set up a timer for start and end time.\n",
    "    \n",
    "    With the python `with open` function, I created the `tweet_json.txt` and wrote the output to it, I appended failed oned to the empty dictionary created bove. I printed `success` for each successful tweet along with the tweet id.\n",
    "    In the end, I printed the time taken and the failed dictionary.\n",
    "    \n",
    "    With the python `with open` function again and a `for loop`, I read the `tweet_json.txt` with `utf-8` encoding line by line and loaded each line as a `json` file. I saved each tweet id, retweet_count and favorite_count which I later converted to a dattaframe named `df3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data\n",
    "\n",
    "I assessed the data visually by calling the data names and scrolling up, down, left and right. I did programmatic assessment with various python/pandas functions and methods such as info(), describe(), head(), isnull(), sample() and shape.\n",
    "\n",
    "I detected about 10 data quality and 2 data tidiness issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "* I first made copies on all data so that I could work on the copies\n",
    "* I converted the timestamp column from string (object) to datetime\n",
    "* I converted tweet_id to string from integer\n",
    "* I dropped rows with retweet\n",
    "* I dropped in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id and retweeted_status_timestamp columns because they have over 90% missing values in each\n",
    "* I dropped rows with missing values in  expanded_urls column because they are few\n",
    "* I combined the four dog stages spread across four columns into one single column\n",
    "* I merged all the data sets on the common column (tweet_id) with inner merging to get common rows in all.\n",
    "* I tested my data cleaning process and made sure it was effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "I saved the merged data in a csv file named `twitter_archive_master.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
